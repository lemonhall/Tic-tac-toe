# RL 模型防守缺失与改进方案备忘

> 时间：2025-11-06  
> 目的：记录当前模型在与真人对弈时出现“未及时阻挡三连”问题的根因、验证实验、改进路线与实施优先级，方便后续决策与迭代。

---
## 1. 问题现象概述
- 与规则对手 (SimpleAI) 或 Minimax 评估时，模型达到高平局率 (>90%)。
- 与真人对弈时：你构造明显“二连 + 空”必胜威胁，它偶尔没有在关键格落子防守，导致被一手取胜。
- 主观直觉：既然平局率高，防守不应如此差。产生困惑。

## 2. 当前对手类型：SimpleAI 机制概述
优先级：
1. 能直接赢 → 赢棋动作。
2. 对手能直接赢 → 阻挡动作。
3. 占中心 `(1,1)`。
4. 占角落（四角随机）。
5. 占边缘（四边随机）。
6. 兜底随机。

缺失：不主动制造“多重威胁”(fork)、不提前铺设“潜在双路线”。因此训练/评估时出现的局面偏“规整”，阻挡场景少且单一。

## 3. 根因分析（为什么真人局面下漏防）
| 维度 | 训练/评估分布 (SimpleAI) | 真人对弈分布 | 影响 |
|------|--------------------------|--------------|------|
| 威胁复杂度 | 单点直接赢/堵 | 可故意延迟完成、构造双威胁 | 模型没见过复杂模式 |
| 中间奖励 | 仅终局 +/- 分 | 缺少“成功防守”即时反馈 | 学不到“防守 ≠ 随便下” |
| 观测特征 | 9 元素 (-1/0/1) | 同 | 没显式标记“紧急格” |
| 探索策略 | 可能随机偏移 (非 deterministic) | 同 | 导致低概率漏防动作被采样 |
| 样本多样性 | 单对手风格 | 自适应、人为诱导 | 分布外行为无体系响应 |

结论：高平局率仅说明“与规则型或近最优策略互不犯错”，并不等价于“掌握威胁识别细粒度技能”。

## 4. 分布迁移 (Distribution Shift)
- 模型大部分梯度更新来自与 SimpleAI/Minimax 的局面集合。
- 真人设计的“钩子局面”（例如延迟第三子、不立即下赢棋点）在训练集中稀缺。
- 因此策略对这类状态下的动作概率分布不够“尖锐”，防守与非防守格概率差距不足。

## 5. 影响因素细分
1. 奖励稀疏：只在输时 -15，错失防守的惩罚迟到。  
2. 没有威胁特征：网络需自己从局面抽象“对手一手即胜”模式，增加学习难度。  
3. 探索噪声：`deterministic=False` 导致策略分布中非最优动作偶尔被采样。  
4. 样本覆盖面狭窄：缺少“复杂威胁生成”对手、缺少 fork 局面。  
5. 缺少防守专项微调：没有监督信号放大“必须防守”场景的梯度。  

## 6. 验证实验设计（建议先观察再大改）
| 编号 | 目标 | 方法 | 指标 |
|------|------|------|------|
| A | 探索是否主因 | 改为 deterministic 推理 20 次防守测试 | 漏防次数减幅 |
| B | 静态防守能力 | 构造 6~8 个威胁局面喂给模型，检查选的动作 | 正确率 (%) |
| C | 对手分布影响 | RL vs SimpleAI 500 局统计“必堵未堵导致直接输” | 漏防率 |
| D | 特征价值 | 临时加手工“威胁掩码”后即时评估 | 防守正确率提升幅度 |

### 静态威胁局面示例（行列用 0-based）：
```
X X _ / _ O _ / _ _ _
O _ _ / _ X X / _ _ _
X _ O / _ X _ / _ _ O
_ _ _ / X O X / O _ _
```
对于每个局面，标记所有“对手一手即胜”格 → 检查模型是否选择其一。

## 7. 低成本临时补救
1. 推理阶段设 `deterministic=True`。  
2. 若发现合法动作集合中存在唯一“must_block”位置，而策略未选中 → 强制替换为防守位。  
3. 打日志：当存在必须防守位却未选中时写入一个 `miss_defense.log`，便于统计。  

## 8. 结构化改进路线
| 阶段 | 目标 | 动作 | 输出 |
|------|------|------|------|
| 1 | 稳定展示 | deterministic + 强制防守 | 漏防显著下降 |
| 2 | 特征增强 | 观测加 9 维 threat_mask | 策略对“紧急”位置概率集中 |
| 3 | 奖励细化 | 防守成功 +3；漏防额外 -8；形成双威胁 +4 | 更快学会权衡防守/进攻 |
| 4 | 多样对手 | Curriculum：SimpleAI+随机+攻击型脚本 | 分布鲁棒性提升 |
| 5 | 监督微调 | 枚举防守局面 → 交叉熵微调 | 防守场景动作准确率提高 |
| 6 | 自博弈扩展 | 保存策略版本作为对手池 | 长期稳定性与探索提升 |

### 特征增强示意
- threat_mask[i] = 1  若：在位置 i 下对手（下一步）能直接获胜，否则 0。  
- 组合输入：`concat([board_vector, threat_mask])` → 18 维。  

### 奖励 shaping 示例
- `if must_block and action == block_pos: reward += 3`
- `if must_block and action != block_pos and opp_wins_next: reward -= 8`
- 注意：保持终局奖励主导，避免 shaping 抹平胜负信号。

## 9. 优先级与投入评估
| 方案 | 投入 (低/中/高) | 预期收益 | 风险 |
|------|-----------------|----------|------|
| deterministic & 强制防守 | 低 | 即刻减少漏防 | 可能掩盖策略真实能力 |
| threat_mask 特征 | 中 | 根治“看不懂威胁” | 需改模型输入 & 重训 |
| 奖励 shaping | 中 | 加速学习防守 | 需要调参防止过拟合防守 |
| 多对手 curriculum | 中 | 广泛场景泛化 | 需实现新对手脚本 |
| 监督防守微调 | 中 | 精准提升防守正确率 | 需要数据生成步骤 |
| 自博弈 + 版本池 | 高 | 长期策略进化 | 架构复杂度上升 |

## 10. 风险与注意事项
- 过度 shaping → 策略偏“被动防守”，进攻效率下降。  
- threat 特征错误标注 → 错误归纳强化“假威胁”。  
- 强制动作覆盖 → 模型真实策略分布难以评估。  
- 多样对手过多随机 → 学习信号稀释，收敛变慢。  

## 11. 快速检查脚本骨架（非代码实现，仅结构）
### `threat_test.py` 结构
```
load model
for board in PREDEFINED_THREAT_BOARDS:
    obs = encode(board, current_player)
    mask = legal_moves(board)
    action = model.predict(obs, action_masks=mask, deterministic=True)
    print(board, action, is_block?)
统计正确率
```

### `compute_threat_mask(board, opponent)` 逻辑要点
```
threat_positions = []
for empty in empties:
  simulate place opponent at empty
  if check_winner(opponent): threat_positions.append(empty)
mask = [1 if pos in threat_positions else 0 for pos in all_9]
```

## 12. 后续决策建议
1. 先仅做静态威胁测试确认真实“看不到”程度。  
2. 若 deterministic 后仍高漏防率 → 立即上 threat_mask。  
3. threat_mask 有效后再考虑奖励 shaping（避免一次性多源变量干扰判断）。  
4. 防守监督数据只在需要进一步“收尾强化”时实施。  

## 13. 一句话总结
当前漏防不是因为内置 SimpleAI 太弱，而是因为训练分布缺少你真人制造的威胁形态 + 特征与奖励都未强调“防守紧急性”，导致策略在新分布下表现退化。优先加 deterministic 展示与威胁特征，再逐步扩展对手与奖励结构。

---
（完）
