"""
å¼ºåŒ–å­¦ä¹ Agent - ä½¿ç”¨Stable-Baselines3
åŸºäºPPOç®—æ³•è®­ç»ƒäº•å­—æ£‹AIï¼Œé€šè¿‡APIæ¥å…¥äº•å­—æ£‹å†³æ–—åœº
"""
import requests
import numpy as np
import time
import gymnasium as gym
from gymnasium import spaces
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import BaseCallback
import os


class TicTacToeEnv(gym.Env):
    """äº•å­—æ£‹å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, base_url='http://127.0.0.1:5000', opponent='ai'):
        super().__init__()
        
        self.base_url = base_url
        self.opponent = opponent  # 'ai' or 'random'
        self.game_id = None
        self.player = None  # 'X' or 'O'
        
        # åŠ¨ä½œç©ºé—´: 9ä¸ªä½ç½® (0-8)
        self.action_space = spaces.Discrete(9)
        
        # çŠ¶æ€ç©ºé—´: 3x3æ£‹ç›˜ï¼Œæ¯ä¸ªä½ç½®å¯ä»¥æ˜¯ 0(ç©º), 1(æˆ‘æ–¹), -1(å¯¹æ–¹)
        # å±•å¹³ä¸º9ç»´å‘é‡
        self.observation_space = spaces.Box(
            low=-1, high=1, shape=(9,), dtype=np.float32
        )
        
        # è®¾ç½®HTTPä¼šè¯
        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=0.05,
                      status_forcelist=[429, 500, 502, 503, 504],
                      allowed_methods=["GET", "POST", "HEAD", "OPTIONS"])
        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_count = 0
        self.wins = 0
        self.losses = 0
        self.draws = 0
        
    def _create_game(self):
        """åˆ›å»ºæ–°æ¸¸æˆ"""
        url = f'{self.base_url}/api/game/create'
        try:
            response = self.session.post(url, json={
                'player_x_type': 'agent',
                'player_o_type': self.opponent
            }, timeout=5)
            
            if response.status_code == 200:
                data = response.json()
                self.game_id = data['game_id']
                self.player = 'X'  # Agentæ€»æ˜¯Xï¼ˆå…ˆæ‰‹ï¼‰
                return True
            else:
                print(f"åˆ›å»ºæ¸¸æˆå¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            print(f"åˆ›å»ºæ¸¸æˆå¼‚å¸¸: {e}")
            return False
    
    def _get_game_state(self):
        """è·å–æ¸¸æˆçŠ¶æ€"""
        if not self.game_id:
            return None
        
        url = f'{self.base_url}/api/game/{self.game_id}/state'
        try:
            response = self.session.get(url, timeout=5)
            if response.status_code == 200:
                return response.json().get('game_state')
            return None
        except Exception as e:
            print(f"è·å–çŠ¶æ€å¼‚å¸¸: {e}")
            return None
    
    def _make_move(self, row, col):
        """æ‰§è¡Œç§»åŠ¨"""
        if not self.game_id:
            return False
        
        url = f'{self.base_url}/api/game/{self.game_id}/move'
        try:
            # ç¡®ä¿ row å’Œ col æ˜¯ Python int ç±»å‹ï¼Œè€Œä¸æ˜¯ numpy int64
            response = self.session.post(url, json={
                'row': int(row),
                'col': int(col)
            }, timeout=5)
            return response.status_code == 200
        except Exception as e:
            print(f"ç§»åŠ¨å¼‚å¸¸: {e}")
            return False
    
    def _request_ai_move(self):
        """è¯·æ±‚AIä¸‹æ£‹"""
        if not self.game_id:
            return False
        
        url = f'{self.base_url}/api/game/{self.game_id}/ai-move'
        try:
            response = self.session.post(url, timeout=5)
            return response.status_code == 200
        except Exception as e:
            print(f"AIç§»åŠ¨å¼‚å¸¸: {e}")
            return False
    
    def _board_to_observation(self, board, player):
        """
        å°†æ£‹ç›˜è½¬æ¢ä¸ºè§‚å¯Ÿå‘é‡
        board: 3x3åˆ—è¡¨ï¼Œå…ƒç´ ä¸º None, 'X', 'O'
        player: å½“å‰Agentçš„ç©å®¶æ ‡è¯† ('X' or 'O')
        è¿”å›: 9ç»´numpyæ•°ç»„ï¼Œ1=æˆ‘æ–¹ï¼Œ-1=å¯¹æ–¹ï¼Œ0=ç©º
        """
        obs = []
        for row in board:
            for cell in row:
                if cell is None:
                    obs.append(0)
                elif cell == player:
                    obs.append(1)  # æˆ‘æ–¹
                else:
                    obs.append(-1)  # å¯¹æ–¹
        return np.array(obs, dtype=np.float32)
    
    def _action_to_position(self, action):
        """å°†åŠ¨ä½œ(0-8)è½¬æ¢ä¸ºæ£‹ç›˜ä½ç½®(row, col)"""
        # ç¡®ä¿è¿”å› Python int ç±»å‹
        return int(action // 3), int(action % 3)
    
    def _is_valid_action(self, action, board):
        """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦åˆæ³•ï¼ˆä½ç½®æ˜¯å¦ä¸ºç©ºï¼‰"""
        row, col = self._action_to_position(action)
        return board[row][col] is None
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–°æ¸¸æˆ"""
        super().reset(seed=seed)
        
        self.episode_count += 1
        
        # åˆ›å»ºæ–°æ¸¸æˆ
        if not self._create_game():
            # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œè¿”å›ç©ºæ£‹ç›˜
            return np.zeros(9, dtype=np.float32), {}
        
        # è·å–åˆå§‹çŠ¶æ€
        game_state = self._get_game_state()
        if game_state:
            board = game_state['board']
            obs = self._board_to_observation(board, self.player)
            return obs, {}
        
        return np.zeros(9, dtype=np.float32), {}
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        game_state = self._get_game_state()
        if not game_state:
            return np.zeros(9, dtype=np.float32), -10, True, False, {}
        
        board = game_state['board']
        
        # æ£€æŸ¥åŠ¨ä½œæ˜¯å¦åˆæ³•
        if not self._is_valid_action(action, board):
            # éæ³•ç§»åŠ¨ï¼Œç»™äºˆæƒ©ç½šå¹¶ç»“æŸ
            obs = self._board_to_observation(board, self.player)
            return obs, -5, True, False, {'illegal_move': True}
        
        # æ‰§è¡Œç§»åŠ¨
        row, col = self._action_to_position(action)
        if not self._make_move(row, col):
            return np.zeros(9, dtype=np.float32), -10, True, False, {}
        
        time.sleep(0.05)  # çŸ­æš‚ç­‰å¾…æœåŠ¡å™¨æ›´æ–°
        
        # è·å–ç§»åŠ¨åçš„çŠ¶æ€
        game_state = self._get_game_state()
        if not game_state:
            return np.zeros(9, dtype=np.float32), -10, True, False, {}
        
        board = game_state['board']
        status = game_state['status']
        
        # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
        if status == 'finished':
            winner = game_state.get('winner')
            obs = self._board_to_observation(board, self.player)
            
            if winner == self.player:
                # èµ¢äº†
                self.wins += 1
                return obs, 10, True, False, {'result': 'win'}
            elif winner is None:
                # å¹³å±€
                self.draws += 1
                return obs, 0, True, False, {'result': 'draw'}
            else:
                # è¾“äº†
                self.losses += 1
                return obs, -10, True, False, {'result': 'loss'}
        
        # æ¸¸æˆç»§ç»­ï¼Œè¯·æ±‚å¯¹æ‰‹ä¸‹æ£‹
        self._request_ai_move()
        time.sleep(0.05)
        
        # è·å–å¯¹æ‰‹ä¸‹æ£‹åçš„çŠ¶æ€
        game_state = self._get_game_state()
        if not game_state:
            return np.zeros(9, dtype=np.float32), -10, True, False, {}
        
        board = game_state['board']
        status = game_state['status']
        obs = self._board_to_observation(board, self.player)
        
        # å†æ¬¡æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
        if status == 'finished':
            winner = game_state.get('winner')
            
            if winner == self.player:
                self.wins += 1
                return obs, 10, True, False, {'result': 'win'}
            elif winner is None:
                self.draws += 1
                return obs, 0, True, False, {'result': 'draw'}
            else:
                self.losses += 1
                return obs, -10, True, False, {'result': 'loss'}
        
        # æ¸¸æˆç»§ç»­ï¼Œç»™äºˆå°å¥–åŠ±ï¼ˆæ´»ç€å°±å¥½ï¼‰
        return obs, 0.1, False, False, {}
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰ï¼‰"""
        if mode == 'human' and self.game_id:
            print(f"æ¸¸æˆID: {self.game_id}")
            print(f"è®¿é—®: {self.base_url}")
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.session.close()


class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹å›è°ƒï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦"""
    
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
    
    def _on_step(self):
        # æ¯100æ­¥æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
        if self.n_calls % 100 == 0:
            # ä» DummyVecEnv ä¸­è·å–è¢«åŒ…è£…çš„ç¯å¢ƒ
            vec_env = self.training_env.envs[0]
            # å¦‚æœç¯å¢ƒè¢« Monitor åŒ…è£…ï¼Œéœ€è¦è®¿é—® .env è·å–åŸå§‹ç¯å¢ƒ
            if hasattr(vec_env, 'env'):
                env = vec_env.env
            else:
                env = vec_env
            
            total = env.wins + env.losses + env.draws
            if total > 0:
                win_rate = env.wins / total * 100
                print(f"\næ­¥æ•°: {self.n_calls} | "
                      f"å›åˆ: {env.episode_count} | "
                      f"èƒœ: {env.wins} | è´Ÿ: {env.losses} | å¹³: {env.draws} | "
                      f"èƒœç‡: {win_rate:.1f}%")
        return True


def train_agent(total_timesteps=10000, model_path='models/rl_agent_ppo'):
    """è®­ç»ƒå¼ºåŒ–å­¦ä¹ Agent"""
    print("="*60)
    print("äº•å­—æ£‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ - Stable-Baselines3 PPO")
    print("="*60)
    print(f"\nè®­ç»ƒæ­¥æ•°: {total_timesteps}")
    print("å¯¹æ‰‹: AI")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}\n")
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒ
    env = TicTacToeEnv(opponent='ai')
    
    # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦ç¬¦åˆGymè§„èŒƒ
    print("æ£€æŸ¥ç¯å¢ƒ...")
    check_env(env, warn=True)
    print("âœ“ ç¯å¢ƒæ£€æŸ¥é€šè¿‡\n")
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    if os.path.exists(f"{model_path}.zip"):
        print(f"åŠ è½½å·²æœ‰æ¨¡å‹: {model_path}.zip")
        model = PPO.load(model_path, env=env)
    else:
        print("åˆ›å»ºæ–°æ¨¡å‹...")
        model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=0.0003,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
        )
    
    # è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...\n")
    callback = TrainingCallback()
    
    # ä¿å­˜å¯¹åŸå§‹ç¯å¢ƒçš„å¼•ç”¨ï¼ˆåœ¨è¢«åŒ…è£…ä¹‹å‰ï¼‰
    original_env = env
    
    model.learn(total_timesteps=total_timesteps, callback=callback)
    
    # ä¿å­˜æ¨¡å‹
    model.save(model_path)
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}.zip")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ï¼ˆä½¿ç”¨åŸå§‹ç¯å¢ƒå¼•ç”¨ï¼‰
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    total = original_env.wins + original_env.losses + original_env.draws
    if total > 0:
        print(f"æ€»å›åˆ: {original_env.episode_count}")
        print(f"èƒœåˆ©: {original_env.wins} ({original_env.wins/total*100:.1f}%)")
        print(f"å¤±è´¥: {original_env.losses} ({original_env.losses/total*100:.1f}%)")
        print(f"å¹³å±€: {original_env.draws} ({original_env.draws/total*100:.1f}%)")
    
    original_env.close()
    return model


def test_agent(model_path='models/rl_agent_ppo', num_games=10):
    """æµ‹è¯•è®­ç»ƒå¥½çš„Agent"""
    print("="*60)
    print("æµ‹è¯•å¼ºåŒ–å­¦ä¹ Agent")
    print("="*60)
    print(f"æ¨¡å‹: {model_path}.zip")
    print(f"æµ‹è¯•å±€æ•°: {num_games}\n")
    
    # åŠ è½½ç¯å¢ƒå’Œæ¨¡å‹
    env = TicTacToeEnv(opponent='ai')
    
    if not os.path.exists(f"{model_path}.zip"):
        print(f"âœ— æ¨¡å‹ä¸å­˜åœ¨: {model_path}.zip")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒ: python rl_agent.py --train")
        return
    
    model = PPO.load(model_path)
    
    # æµ‹è¯•
    wins = 0
    losses = 0
    draws = 0
    
    for i in range(num_games):
        obs, _ = env.reset()
        done = False
        
        print(f"\nç¬¬ {i+1} å±€æ¸¸æˆ (ID: {env.game_id})")
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            
            if done:
                result = info.get('result', 'unknown')
                if result == 'win':
                    wins += 1
                    print(f"  âœ“ èƒœåˆ©ï¼å¥–åŠ±: {reward}")
                elif result == 'loss':
                    losses += 1
                    print(f"  âœ— å¤±è´¥ã€‚å¥–åŠ±: {reward}")
                elif result == 'draw':
                    draws += 1
                    print(f"  - å¹³å±€ã€‚å¥–åŠ±: {reward}")
                elif info.get('illegal_move'):
                    losses += 1
                    print(f"  âœ— éæ³•ç§»åŠ¨ã€‚å¥–åŠ±: {reward}")
        
        time.sleep(1)  # æ¸¸æˆé—´éš”
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    print(f"æ€»å±€æ•°: {num_games}")
    print(f"èƒœåˆ©: {wins} ({wins/num_games*100:.1f}%)")
    print(f"å¤±è´¥: {losses} ({losses/num_games*100:.1f}%)")
    print(f"å¹³å±€: {draws} ({draws/num_games*100:.1f}%)")
    
    env.close()


def play_interactive(model_path='models/rl_agent_ppo'):
    """äº¤äº’å¼å¯¹æˆ˜ - æŒç»­ç©æ¸¸æˆ"""
    print("="*60)
    print("å¼ºåŒ–å­¦ä¹ Agent - è¿ç»­å¯¹æˆ˜æ¨¡å¼")
    print("="*60)
    print(f"æ¨¡å‹: {model_path}.zip")
    print("æŒ‰ Ctrl+C é€€å‡º\n")
    
    # åŠ è½½ç¯å¢ƒå’Œæ¨¡å‹
    env = TicTacToeEnv(opponent='ai')
    
    if not os.path.exists(f"{model_path}.zip"):
        print(f"âœ— æ¨¡å‹ä¸å­˜åœ¨: {model_path}.zip")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒ: python rl_agent.py --train")
        return
    
    model = PPO.load(model_path)
    
    game_count = 0
    
    try:
        while True:
            game_count += 1
            print(f"\n{'='*60}")
            print(f"ç¬¬ {game_count} å±€æ¸¸æˆ")
            print(f"{'='*60}")
            
            obs, _ = env.reset()
            done = False
            
            print(f"æ¸¸æˆID: {env.game_id}")
            print(f"è®¿é—® {env.base_url} è§‚çœ‹å¯¹æˆ˜\n")
            
            step_count = 0
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = env.step(action)
                step_count += 1
                
                if done:
                    result = info.get('result', 'unknown')
                    if result == 'win':
                        print(f"ğŸ‰ ç¬¬ {game_count} å±€: èƒœåˆ©ï¼({step_count} æ­¥)")
                    elif result == 'loss':
                        print(f"ğŸ˜¢ ç¬¬ {game_count} å±€: å¤±è´¥ã€‚({step_count} æ­¥)")
                    elif result == 'draw':
                        print(f"ğŸ¤ ç¬¬ {game_count} å±€: å¹³å±€ã€‚({step_count} æ­¥)")
            
            # æ˜¾ç¤ºç´¯è®¡ç»Ÿè®¡
            total = env.wins + env.losses + env.draws
            if total > 0:
                print(f"\nç´¯è®¡ç»Ÿè®¡: èƒœ {env.wins} | è´Ÿ {env.losses} | å¹³ {env.draws} | "
                      f"èƒœç‡ {env.wins/total*100:.1f}%")
            
            # ç­‰å¾…ä¸‹ä¸€å±€
            time.sleep(3)
    
    except KeyboardInterrupt:
        print(f"\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º (å…±ç©äº† {game_count} å±€)")
        total = env.wins + env.losses + env.draws
        if total > 0:
            print(f"æœ€ç»ˆç»Ÿè®¡: èƒœ {env.wins} | è´Ÿ {env.losses} | å¹³ {env.draws} | "
                  f"èƒœç‡ {env.wins/total*100:.1f}%")
    
    env.close()


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == '--train':
            # è®­ç»ƒæ¨¡å¼
            timesteps = int(sys.argv[2]) if len(sys.argv) > 2 else 10000
            train_agent(total_timesteps=timesteps)
        
        elif command == '--test':
            # æµ‹è¯•æ¨¡å¼
            num_games = int(sys.argv[2]) if len(sys.argv) > 2 else 10
            test_agent(num_games=num_games)
        
        elif command == '--play':
            # è¿ç»­å¯¹æˆ˜æ¨¡å¼
            play_interactive()
        
        else:
            print("æœªçŸ¥å‘½ä»¤ï¼")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  è®­ç»ƒ: python rl_agent.py --train [æ­¥æ•°]")
            print("  æµ‹è¯•: python rl_agent.py --test [å±€æ•°]")
            print("  å¯¹æˆ˜: python rl_agent.py --play")
    
    else:
        # é»˜è®¤ï¼šè®­ç»ƒæ¨¡å¼
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  è®­ç»ƒ: python rl_agent.py --train [æ­¥æ•°]")
        print("  æµ‹è¯•: python rl_agent.py --test [å±€æ•°]")
        print("  å¯¹æˆ˜: python rl_agent.py --play")
        print("\nè¿è¡Œé»˜è®¤è®­ç»ƒ...")
        train_agent(total_timesteps=5000)
