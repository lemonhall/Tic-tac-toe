# 井字棋决斗场 (Tic-Tac-Toe Arena)

## 📋 项目概述

**井字棋决斗场** 是一个功能完整的井字棋游戏平台，专为 AI Agent 对战、模型训练和博弈研究而设计。该项目整合了传统游戏AI算法（Minimax、启发式策略）与现代强化学习（PPO、MaskablePPO），提供了一套完整的游戏对战、训练、评估和观战系统。

### 核心特性

- 🎮 **多模式游戏**：人类 vs 人类、人类 vs AI、AI vs AI、观战模式
- 🤖 **多类型玩家**：人类、规则AI、强化学习Agent、外部API接入
- 🧠 **AI策略库**：Minimax算法（困难）、启发式策略（简单/中等）、强化学习模型
- 📊 **强化学习支持**：PPO、MaskablePPO算法，支持训练和部署
- 🔌 **完整API体系**：RESTful API + SSE事件流，支持异步操作
- 📈 **观战与回放**：支持游戏列表、时间线观战、变速回放
- 🎯 **性能优化**：事件队列、自动清理机制、延迟测试工具

---

## 🏗️ 项目架构

### 整体设计

```
┌──────────────────────────────────────────────────────────────┐
│                     井字棋决斗场平台                           │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────────┐           ┌─────────────────────┐  │
│  │   前端 (Web UI)     │   HTTP    │   后端 (Flask API)  │  │
│  │                     │◄─────────►│                     │  │
│  │ - Vue/原生JS       │   SSE     │ - 游戏逻辑         │  │
│  │ - 实时渲染         │  Events   │ - AI决策           │  │
│  │ - 玩家交互         │           │ - 状态管理         │  │
│  └─────────────────────┘           └─────────────────────┘  │
│                                             ▲                 │
│                                             │                 │
│                    ┌────────────────────────┴────────────┐   │
│                    │                                     │    │
│           ┌────────▼─────────┐            ┌────────────▼──┐  │
│           │  AI 策略引擎     │            │  RL 模型库    │  │
│           │                  │            │              │  │
│           │ - Minimax        │            │ - PPO Model  │  │
│           │ - SimpleAI       │            │ - MaskablePPO│  │
│           │ - TicTacToeAI    │            │ - 训练脚本   │  │
│           └──────────────────┘            └──────────────┘  │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

### 后端架构

```
app.py (Flask 主应用)
    ├─ /api/game/create          → 创建游戏
    ├─ /api/game/<id>/state      → 获取游戏状态
    ├─ /api/game/<id>/move       → 执行下棋
    ├─ /api/game/<id>/ai-move    → AI自动下棋
    ├─ /api/game/<id>/events     → SSE 事件流
    ├─ /api/games                → 游戏列表 & 观战
    └─ /api/game/<id>/timeline-stream  → 时间线回放

game_logic.py (核心游戏类)
    └─ TicTacToeGame
        ├─ make_move()           → 下棋逻辑
        ├─ check_winner()        → 胜负判定
        ├─ get_state()           → 获取状态
        └─ clone()               → 游戏克隆（用于AI评估）

game_manager.py (游戏实例管理)
    └─ GameManager
        ├─ create_game()         → 创建新游戏实例
        ├─ get_game()            → 获取游戏实例
        ├─ make_move()           → 执行移动并发出事件
        └─ 自动清理机制          → TTL 和过期游戏清理

ai_strategy.py (AI决策引擎)
    ├─ SimpleAI                  → 简单AI（优先级策略）
    └─ TicTacToeAI              → 高级AI
        ├─ _get_random_move()    → 随机移动（简单难度）
        ├─ _get_minimax_move()   → Minimax算法（困难难度）
        └─ _minimax()            → Alpha-Beta剪枝实现

rl_agent.py (强化学习 v1)
    └─ TicTacToeEnv (Gym环境)
        ├─ PPO 训练
        └─ API 对战

rl_agent_v2.py (强化学习 v2)
    └─ TicTacToeEnv (支持动作掩码)
        ├─ MaskablePPO 训练
        └─ 自定义奖励函数
```

### 前端架构

```
index.html (页面骨架)
    │
    ├─ CSS 样式系统
    │   ├─ reset.css         → 重置样式
    │   ├─ layout.css        → 布局与容器
    │   ├─ board.css         → 棋盘样式
    │   ├─ controls.css      → 控制按钮
    │   ├─ status.css        → 状态指示器
    │   └─ modal.css         → 模态框
    │
    └─ JavaScript 模块系统
        ├─ main.js           → 应用入口
        ├─ gameController.js  → 主控制器
        │   ├─ 游戏模式管理
        │   ├─ 游戏流程控制
        │   ├─ 事件监听
        │   └─ 观战逻辑
        │
        ├─ gameState.js       → 状态管理
        │   ├─ 棋盘状态
        │   ├─ 玩家类型
        │   ├─ 移动历史
        │   └─ 统计数据
        │
        ├─ api.js            → API 客户端
        │   ├─ 游戏创建/状态
        │   ├─ 移动执行
        │   ├─ SSE 事件监听
        │   ├─ 游戏列表查询
        │   └─ 时间线流处理
        │
        ├─ ui.js             → UI 管理器
        │   ├─ 棋盘渲染
        │   ├─ 状态更新
        │   ├─ 模态框显示
        │   ├─ 音效控制
        │   └─ 视觉反馈
        │
        └─ events.js         → 事件处理
            ├─ 事件绑定
            ├─ 手势识别
            ├─ 键盘快捷键
            └─ 事件代理
```

---

## 📁 核心文件说明

### 游戏核心

| 文件 | 功能 | 关键类/函数 |
|------|------|-----------|
| `game_logic.py` | 游戏规则与状态 | `TicTacToeGame` |
| `game_manager.py` | 多游戏管理 | `GameManager` |
| `ai_strategy.py` | AI决策算法 | `SimpleAI`, `TicTacToeAI` |

### 后端服务

| 文件 | 功能 | 说明 |
|------|------|-----|
| `app.py` | Flask 应用主文件 | RESTful API、SSE事件、静态文件服务 |
| `config.json` | 配置文件 | 服务器、游戏、特性配置 |

### 强化学习

| 文件 | 版本 | 特点 |
|------|------|------|
| `rl_agent.py` | v1 | 标准 PPO 环境 |
| `rl_agent_v2.py` | v2 | 支持动作掩码，优化非法移动处理 |
| `play_against_rl.py` | 推理工具 | 加载训练好的模型进行对战 |

### 训练与工具

| 文件 | 用途 |
|------|------|
| `train_rl.ps1` | 训练脚本（PowerShell） |
| `train_v2.ps1` | V2版本训练 |
| `train_with_human.py` | 人类标注训练 |
| `training_monitor.py` | 训练监视器 |
| `benchmark_agents.py` | 性能基准测试 |
| `bench_latency.py` | 延迟测试 |

### 前端代码

| 文件 | 功能 |
|------|------|
| `index.html` | 页面结构 |
| `static/js/main.js` | 应用入口 |
| `static/js/gameController.js` | 主要控制逻辑（720行） |
| `static/js/gameState.js` | 状态管理 |
| `static/js/api.js` | API 客户端 |
| `static/js/ui.js` | UI 更新与渲染 |
| `static/js/events.js` | 事件处理 |

### 文档

| 文件 | 内容 |
|------|------|
| `API.md` | 完整 API 文档（484行） |
| `OVERVIEW.md` | 项目概览与架构 |
| `IMPROVEMENTS.md` | 观战模式改进说明 |
| `RL_SUMMARY.md` | 强化学习总结 |
| `防守改进方案.md` | RL 模型防守问题分析 |

---

## 🚀 快速开始

### 1. 环境准备

```bash
# 安装依赖
pip install -r requirements.txt

# 可选：强化学习依赖
pip install -r requirements-rl.txt
```

**主要依赖：**
- Flask 3.0.0 + Flask-CORS
- Numpy 2.3.4 + Gymnasium 1.2.2
- Stable-Baselines3 2.7.0 + SB3-Contrib 2.7.0
- TensorBoard 2.20.0

### 2. 启动服务

```bash
# 方式一：Python 直接运行
python app.py

# 方式二：PowerShell 脚本
./start.ps1
```

### 3. 访问平台

打开浏览器访问：**http://localhost:5000**

---

## 🎮 使用方式

### 游戏模式

#### 1️⃣ 人类 vs 人类
- 两个真人玩家轮流下棋
- 鼠标点击空位落子

#### 2️⃣ 人类 vs AI
- 人类 (X) 先手，AI (O) 自动应答
- 支持三种难度：简单、中等、困难

#### 3️⃣ AI vs AI
- 两个 AI 自动对弈
- 自动进行，可变速回放

#### 4️⃣ 观战模式
- 从游戏列表中选择进行中的游戏实时观看
- 或观看已完成游戏的时间线回放
- 支持 0.5x ~ 2.0x 倍速调整

### API 调用示例

#### 创建游戏
```bash
curl -X POST http://localhost:5000/api/game/create \
  -H "Content-Type: application/json" \
  -d '{"player_x_type": "human", "player_o_type": "ai"}'
```

#### 执行下棋
```bash
curl -X POST http://localhost:5000/api/game/{game_id}/move \
  -H "Content-Type: application/json" \
  -d '{"row": 1, "col": 1}'
```

#### 获取游戏状态
```bash
curl http://localhost:5000/api/game/{game_id}/state
```

#### 获取游戏列表（用于观战）
```bash
curl "http://localhost:5000/api/games?status=in_progress"
```

---

## 🧠 AI 策略详解

### SimpleAI（简单AI）

**决策优先级：**
1. **能赢** → 直接下赢棋点
2. **要堵** → 对手一手即胜则堵
3. **占中心** → 优先中心 (1,1)
4. **占角落** → 四个角随机选择
5. **占边缘** → 四条边随机选择
6. **兜底** → 完全随机

**特点：** 规则简单、决策快速、缺乏进攻性

### TicTacToeAI（高级AI）

**难度参数：**
- `easy`：随机移动
- `medium`：50% Minimax + 50% 随机
- `hard`：完整 Minimax + Alpha-Beta 剪枝

**Minimax 实现：**
```
Minimax(state, depth, isMaximizing, alpha, beta)
├─ 检查终止条件（胜/负/平）
├─ 遍历所有可能移动
│  ├─ 模拟移动（克隆游戏状态）
│  ├─ 递归评估
│  ├─ Alpha-Beta 剪枝
│  └─ 返回评分
└─ 选择最高分移动
```

**评分函数：**
```
赢棋 (当前玩家)    → +10
输棋 (对手)        → -10
平局              → 0
中间状态          → 基于递归深度调整
```

### 强化学习 Agent

**V1 (rl_agent.py)：**
- PPO 算法
- 标准 Gym 环境
- 对手：SimpleAI 或 Minimax

**V2 (rl_agent_v2.py)：** ⭐ 推荐
- MaskablePPO 算法
- **动作掩码**（解决非法移动问题）
- **自定义奖励函数**：防守/进攻激励
- 更高的训练效率

**模型库（models/）：**
```
├─ rl_agent_ppo.zip                    → 原始 v1 模型
├─ rl_agent_v2_ppo.zip                → V2 基础模型
├─ rl_agent_v2_ppo_进攻达人_3%.zip    → 进攻倾向版本
└─ rl_agent_v2_ppo_防守大师_90%.zip   → 防守倾向版本
```

---

## 📊 强化学习训练

### 训练脚本

```bash
# 方式一：直接 Python
python train_rl.py

# 方式二：PowerShell（推荐）
./train_rl.ps1
./train_v2.ps1
```

### 训练配置

**RL 配置文件：** `rl_config.json`

```json
{
  "learning_rate": 0.0003,
  "n_steps": 2048,
  "batch_size": 64,
  "n_epochs": 10,
  "gamma": 0.99,
  "gae_lambda": 0.95,
  "opponent": "ai",  // "ai" 或 "random"
  "total_timesteps": 100000
}
```

### 模型评估

```bash
python benchmark_agents.py      # 对各AI进行基准测试
python play_against_rl.py       # 人类与 RL 模型对战
```

---

## 🔌 API 完整参考

### 端点列表

| 方法 | 端点 | 功能 |
|------|------|------|
| GET | `/api/health` | 健康检查 |
| POST | `/api/game/create` | 创建新游戏 |
| GET | `/api/game/<id>/state` | 获取游戏状态 |
| POST | `/api/game/<id>/move` | 执行下棋 |
| POST | `/api/game/<id>/ai-move` | AI 自动下棋 |
| GET | `/api/game/<id>/events` | SSE 事件流 |
| GET | `/api/game/<id>/timeline-stream` | 时间线回放流 |
| GET | `/api/games` | 游戏列表（支持状态过滤） |
| POST | `/api/game/<id>/reset` | 重置游戏 |

### 请求/响应格式

#### 创建游戏
```json
{
  "player_x_type": "human|ai|agent",
  "player_o_type": "human|ai|agent"
}
```

响应：
```json
{
  "status": "success",
  "game_id": "uuid",
  "game_state": { /* 完整游戏状态 */ }
}
```

#### 下棋
请求：
```json
{
  "row": 0,
  "col": 1,
  "player_id": null
}
```

响应：
```json
{
  "status": "success",
  "result": {
    "success": true,
    "game_over": false,
    "next_player": "O"
  },
  "game_state": { /* 更新后的游戏状态 */ }
}
```

#### 游戏状态
```json
{
  "game_id": "uuid",
  "board": [["X", null, "O"], ...],
  "current_player": "X",
  "status": "in_progress|finished",
  "winner": null,
  "winning_line": [[0,0], [1,1], [2,2]],
  "move_count": 3,
  "player_x_type": "human",
  "player_o_type": "ai"
}
```

---

## 🎯 观战与回放系统

### 功能特性

1. **实时观战**
   - 访问进行中的游戏列表
   - 选择并实时跟踪进展
   - 自动刷新状态

2. **时间线回放**
   - 获取已完成游戏的完整移动历史
   - 支持变速播放（0.5x ~ 2.0x）
   - 显示每步落子信息

3. **自动观战**
   - 后台自动收集整局记录
   - 结束后按指定倍速回放

### 内存管理

**自动清理机制：**
- TTL 设置：已完成游戏 120 分钟后自动删除
- 触发时机：每创建新游戏时检查
- 保留数量：保留最近 20 个已完成游戏

---

## 🔍 性能与优化

### 性能测试工具

```bash
# 基准测试：各 AI 赢率/平率
python benchmark_agents.py

# 延迟测试：API 响应时间
python bench_latency.py
```

### 优化特性

1. **算法优化**
   - Minimax 使用 Alpha-Beta 剪枝
   - 首步优化（中心优先）

2. **网络优化**
   - SSE 事件流减少轮询
   - 事件队列异步处理

3. **内存优化**
   - 自动清理过期游戏
   - 游戏状态序列化存储

---

## 🐛 已知问题与改进方向

### RL 模型防守问题

**问题描述：**
- 与规则 AI 平局率 >90%
- 与真人对弈时偶现漏防现象
- 原因：防守细粒度特征未充分学习

**改进方案（见 `防守改进方案.md`）：**
1. 改为确定性推理降低探索噪声
2. 添加威胁特征显式标记
3. 引入防守专项微调
4. 采用更强的对手进行训练

### 正在进行的优化

- ✅ 动作掩码优化（已完成 V2）
- ✅ 自定义奖励函数设计
- 🔄 防守倾向模型训练
- 🔄 进攻倾向模型训练

---

## 📦 依赖列表

### 运行时依赖
```
Flask==3.0.0
Flask-CORS==4.0.0
Werkzeug==3.0.1
Numpy>=2.3.4
Requests==2.31.0
sseclient-py==1.8.0
```

### 强化学习依赖
```
Gymnasium>=1.2.2
Stable-Baselines3>=2.7.0
SB3-Contrib>=2.7.0
TensorBoard>=2.20.0
```

### 开发工具
```
Python >=3.12
UV (包管理器)
```

---

## 🤝 项目结构总结

```
Tic-tac-toe/
├── 📄 核心游戏逻辑
│   ├─ game_logic.py           游戏规则与状态
│   ├─ game_manager.py         游戏实例管理
│   └─ ai_strategy.py          AI决策
│
├── 🌐 后端服务
│   ├─ app.py                  Flask 应用
│   ├─ config.json             配置文件
│   └─ requirements.txt        依赖列表
│
├── 🤖 强化学习
│   ├─ rl_agent.py             PPO v1 环境
│   ├─ rl_agent_v2.py          MaskablePPO v2 环境
│   ├─ play_against_rl.py      模型推理工具
│   ├─ training_monitor.py     训练监视
│   ├─ train_rl.ps1            v1 训练脚本
│   ├─ train_v2.ps1            v2 训练脚本
│   ├─ train_with_human.py     人类标注训练
│   └─ models/                 已训练模型库
│
├── 💻 前端应用
│   ├─ index.html              页面入口
│   ├─ static/
│   │  ├─ js/
│   │  │  ├─ main.js           应用入口
│   │  │  ├─ gameController.js 主控制器 (720L)
│   │  │  ├─ gameState.js      状态管理
│   │  │  ├─ api.js            API 客户端
│   │  │  ├─ ui.js             UI 管理 (329L)
│   │  │  └─ events.js         事件处理
│   │  └─ css/                 样式文件（6个）
│   │
│   └─ sounds/                 音效资源
│
├── 🧪 测试与工具
│   ├─ test_game.py            游戏逻辑测试
│   ├─ demo.py                 快速演示
│   ├─ demo_rl.ps1             RL 演示
│   ├─ check_rl_setup.py       环境检查
│   ├─ benchmark_agents.py     性能基准
│   └─ bench_latency.py        延迟测试
│
├─ 🚀 启动脚本
│   ├─ start.ps1               启动服务
│   └─ play_against_rl.py      与 RL 对战
│
└─ 📚 文档
    ├─ README.md               (已过期)
    ├─ API.md                  API 文档 (484L)
    ├─ OVERVIEW.md             项目概览
    ├─ IMPROVEMENTS.md         观战改进
    ├─ RL_SUMMARY.md           RL 总结
    ├─ RL_QUICKSTART.md        RL 快速开始
    ├─ HUMAN_TRAINING_NOTES.md 人类训练笔记
    ├─ FLOW.md                 流程图
    ├─ QUICKSTART.md           快速开始
    ├─ OVERVIEW.md             项目概览
    ├─ RL_AGENT_README.md      RL Agent 说明
    └─ 防守改进方案.md         RL 防守问题分析
```

---

## ⚙️ 配置说明

### 服务器配置 (config.json)
```json
{
  "server": {
    "host": "0.0.0.0",
    "port": 5000,
    "debug": true
  },
  "game": {
    "board_size": 3,
    "ai_difficulty": "hard"  // easy, medium, hard
  },
  "features": {
    "enable_sse": true,
    "enable_cors": true,
    "log_level": "INFO"
  }
}
```

### RL 配置 (rl_config.json)
```json
{
  "learning_rate": 0.0003,
  "n_steps": 2048,
  "batch_size": 64,
  "opponent": "ai",
  "total_timesteps": 100000
}
```

---

## 🎓 学习资源

### 推荐阅读

1. **API 系统** → 查看 `API.md`（484行完整文档）
2. **RL 模型** → 查看 `RL_SUMMARY.md` 和 `防守改进方案.md`
3. **快速上手** → 查看 `QUICKSTART.md` 和 `RL_QUICKSTART.md`
4. **架构理解** → 查看 `OVERVIEW.md`

### 代码入口

- **启动应用**：从 `app.py` 开始
- **游戏逻辑**：从 `game_logic.py` 开始
- **前端交互**：从 `static/js/gameController.js` 开始
- **AI 决策**：从 `ai_strategy.py` 开始
- **RL 训练**：从 `rl_agent_v2.py` 开始

---

## 💡 使用技巧

### 1. 开发调试
```bash
# 启用调试模式
export FLASK_ENV=development
export FLASK_DEBUG=1
python app.py
```

### 2. 模型训练与评估
```bash
# 训练新模型（v2）
./train_v2.ps1

# 评估所有 AI
python benchmark_agents.py

# 与训练好的模型对战
python play_against_rl.py
```

### 3. 性能测试
```bash
# 测试 API 延迟
python bench_latency.py

# 监视训练进度
tensorboard --logdir=./logs
```

### 4. 观战模式使用
- 选择"观战"模式
- 点击"开始游戏"获取游戏列表
- 从列表中选择进行中的游戏
- 实时观看或选择已完成游戏进行回放
- 调整回放倍速（0.5x 至 2.0x）

---

## 🔗 相关资源

- **Gymnasium**：https://gymnasium.farama.org/
- **Stable-Baselines3**：https://stable-baselines3.readthedocs.io/
- **Flask**：https://flask.palletsprojects.com/
- **Server-Sent Events**：https://html.spec.whatwg.org/multipage/server-sent-events.html

---

## 📝 更新日志

### 最近更新

- ✅ **V2 强化学习**：MaskablePPO + 动作掩码支持
- ✅ **观战系统**：游戏列表、实时观战、时间线回放
- ✅ **内存优化**：自动清理机制、TTL 管理
- ✅ **前端重构**：模块化 JavaScript、事件系统
- 🔄 **RL 防守优化**：正在进行中

### 开发中功能

- 🔜 强化学习模型对比
- 🔜 多 Agent 对战模式
- 🔜 Web 界面模型选择
- 🔜 训练数据可视化

---

## 📞 项目信息

- **项目名称**：Tic-Tac-Toe Arena (井字棋决斗场)
- **版本**：1.0.0
- **Python 版本**：>=3.12
- **开源协议**：MIT
- **作者**：lemonhall
- **仓库**：Tic-tac-toe (main 分支)

---

## 🚀 下一步

1. **运行服务** → `./start.ps1` 或 `python app.py`
2. **体验游戏** → http://localhost:5000
3. **查看 API** → 参考 `API.md`
4. **训练模型** → 参考 `RL_QUICKSTART.md`
5. **与 AI 对战** → `python play_against_rl.py`

祝您使用愉快！🎮
