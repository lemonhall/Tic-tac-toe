# 人类对弈训练备忘录

> 目的：防止遗忘当前 `train_with_human.py` 的局限、改进方向，以及三条策略路线（A / B / C）。

## 当前脚本现状 (`train_with_human.py`)
- 作用：让你在浏览器创建“外部Agent vs 人类”局面，Agent 作为 X 先手，打完一局后调用 `model.learn(500)`。
- 保存机制（已改）：每局结束立即保存主模型 + 生成带局号 checkpoint（`models/human_sessions/rl_agent_v2_ppo_gXXXX.zip`），并打印时间戳与哈希。
- 奖励：胜 +20，平 +2，负 -15，非法走 -5，轮到自己继续下时 +0.1（微弱 shaping）。
- 问题：你打的那一局的真实交互轨迹（状态/动作/奖励序列）没有直接被 PPO 用来学习，只是触发了后续再次采样的强化学习循环。

## 关键局限
1. 人类下过的轨迹未被保存（缺少监督 / 复现数据）。
2. `model.learn()` 会重新调用 env 产生新 episode；你刚结束的那一局的状态分布没有被直接利用。
3. 交互效率低：每调用一次 learn 需要你重新创建新游戏（阻塞采样）。
4. 没有区分“人类风格模仿”与“最终策略优化”的阶段。
5. 没有策略行为度量（例如胜率曲线、策略熵、动作分布）。

## 三条改进路线选项
### A. 监督学习 / 模仿学习 (Imitation / Behavioral Cloning)
**目标**：先快速“记住你的落子风格”，再在此基础上用 RL 微调。
- 做法：在 `step()` 中记录 `(board_before, agent_action, result_tag)` 到内存列表；每局结束写入 JSON / CSV / Parquet。
- 训练：单独写 `imitation_train.py`，用一个简单 MLP 预测 9 个格子的落子概率（交叉熵）。
- 优点：样本利用率高，收敛快；你下的每步都被学习。
- 缺点：不直接优化最终胜率（可能只是复制你当前策略）。
- 增强：加入“合法动作掩码”，损失只在合法位置归一化；可加数据增强（旋转 / 镜像棋盘）。

### B. 在线人类交互强化学习 (Human-in-the-loop PPO)
**目标**：让人类对弈轨迹直接进入 PPO 的 rollout。
- 做法：改写环境，使 `reset()` 不主动等待你手动建局；而是在真正需要新 episode 时提示浏览器创建；每个 `step()` 的 transition `(s,a,r,s',done)` 直接进入 PPO buffer。
- 采样：设 `n_steps` 足够小（比如 32 或 64）避免长时间阻塞；或者并行开多个“虚拟对弈 + 你真实对弈”的混合环境。
- 优点：学习直接针对奖励优化；补充策略探索。
- 缺点：人工交互慢，PPO 需要大量样本；体验卡顿。
- 增强：用自我对弈 / 旧版本对弈填充样本，与你的对弈样本混合（优先级加权）。

### C. 轨迹重放 / 离线强化学习 (Replay + Offline RL)
**目标**：把你玩过的对局“录制”为轨迹，再用离线或半离线方式训练。
- 做法：存 `episode = [(s0,a0,r1,s1), ..., (sT,aT,rT+1,terminal)]`；写一个离线 Buffer Loader；使用 DQN / BCQ / CQL 等适合离线数据的算法，或对 PPO 做“固定旧策略采样”近似。
- 优点：不阻塞交互；可集中训练；可过滤劣质样本。
- 缺点：离线 RL 对奖励稀疏较敏感；实现复杂度高于 A。
- 增强：混合少量在线自我对弈数据，缓解分布偏移。

## 选择建议
| 目标 | 推荐路径 | 原因 |
|------|----------|------|
| 快速让它“像你一样下” | A | 实现快，直接利用你的每一步 |
| 实时强化对弈优化胜率 | B | 真正的在线学习闭环 |
| 不打扰你、批量慢慢优化 | C | 脱耦数据采集与训练 |

建议顺序：先 A（收集 + 模仿），再 B（在线微调），最后按需加 C 做长期积累与版本回溯。

## 近期实施步骤（建议）
1. 在 `HumanOpponentEnv.step` 里增加轨迹记录列表 `self.episode_transitions`。
2. 在游戏结束处将整局写入 `data/human_sessions/<timestamp>_gXXXX.json`。
3. 编写 `imitation_dataset.py`：读取所有 JSON → 产出 `(board_vector, action_index)`。
4. 编写 `imitation_train.py`：简单 PyTorch 模型（2 层 MLP），训练若干 epoch → 输出 `models/imitation_seed_v1.pt`。
5. 用训练好的模仿策略初始化 PPO 的 policy 权重（手动赋值或通过行为克隆预热）。
6. 调整当前脚本：去掉局后 `model.learn()`，改为“在线 PPO 采样”版本或转为纯采集模式。

## 技术要点备忘
- 棋盘增强：旋转 90/180/270°、水平/垂直翻转 → 位置与 action 同步变换。
- 合法动作掩码：监督训练时对非法格子 logits 置极小值，或在loss时只选合法集合。
- 数据质量标记：结果 `win/draw/loss` 加入样本，用于加权（比如赢的样本 loss 权重高）。
- 版本化：对 imitation 模型使用 `models/imitation/YYYYMMDD_HHMM_vX.pt`。
- 哈希校验：保存后用 SHA256 前 8 位记录到日志，方便回溯。

## 可能的后续增强
- 引入“对手模型池”：与不同旧版本自我对弈，提升泛化。
- 终局奖励改进：在接近胜利的两连时给 +α shaping；阻止对手两连给 +β。
- 策略可视化：记录每步动作概率分布，生成热力图。
- 评估脚本：`evaluate_vs_minimax.py` 定期跑 100 局统计胜/平/负曲线。

## 风险与注意事项
- 监督学习过拟合你的局部习惯，可能忽略全局最优阻截。
- 在线 PPO 卡在“人类太慢”节奏 → 需混合自我对弈样本。
- 离线 RL 容易出现 OOD 行为；策略要限制对不熟悉状态的置信度（可加行为克隆约束）。

## 快速对照摘要
- A = "记你的棋"（数据集 + 监督训练）。
- B = "即时学"（把你纳入采样环）。
- C = "录下来，之后慢慢练"（轨迹 → 离线 / 再训练）。

---
*创建时间：{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*

---

## 真人对弈与内置 AI 自对弈的区别（深入解析）

### 1. 标准 PPO 流程回顾
```
loop:
	collect rollout: (s,a,r,s',done) * N  （快速、连续、无阻塞）
	compute advantages & value loss
	update policy
```
采样与学习紧耦合：谁产生轨迹，谁立刻被用于梯度更新。

### 2. 自对弈（与程序内 AI）特点
- 环境纯代码驱动，步进毫秒级。
- 每次 `env.step()` 立即返回下一个状态。
- Rollout 高速积累 → 批量更新。
- 轨迹天然进入 PPO buffer（无人工延迟）。

### 3. 目前真人对弈脚本的行为
- `step()` 里等待人类手动走棋（sleep + 轮询）。
- 一局结束后才调用 `model.learn(500)`，而 learn 会再次触发新的采样过程（需要你再建新局）。
- 那一局的真实轨迹（你的落子 + Agent 反应）没有缓存与复用。
- 所以：你“陪它玩”这一局，只是间接帮它开始下一个慢速采样阶段。

### 4. 为什么说“没有真正学你的棋”
| 维度 | 自对弈 | 真人对弈（当前） |
|------|--------|------------------|
| 轨迹进入 buffer | 是 | 否（未记录） |
| 采样速度 | 快 | 慢（秒级） |
| 你决策的动作被学习 | 不需要（是程序 AI） | 没有（丢失） |
| `learn()` 用的轨迹 | 刚收集的高速数据 | 重新采样的新局，不是上一局 |

### 5. 结果是什么
- 模型仍可能变化，但极度缓慢，更多取决于后面 500 timesteps 内你是否继续交互。
- 你的思考价值没有进入参数更新路径。

### 6. 真实想达到的“教它”需要什么
要把你的决策变成学习信号，需：
1. 记录 `(state_before, agent_action, reward, state_after, done)` 序列。
2. 结束时形成一个 episode list 写入文件。
3. 用这些 episode 做：
	 - 监督学习（模仿你的落子分布）。
	 - 或离线 RL / 混合在线 RL。

### 7. 三种改善路径关联
- A：解决“记不住你的棋”→ 数据集 + 行为克隆。
- B：解决“轨迹不进 buffer”→ 把真人互动嵌入 rollout。
- C：解决“效率与长期积累”→ 脱耦采集和训练。

### 8. 极简最先加的 3 个改动
```python
# 在环境初始化：
self.episode_transitions = []

# 在 agent 或人类一步落子前后：
self.episode_transitions.append({
	'board': copy_board(),
	'action': action_idx,
	'player': current_player,
	'reward': reward,
})

# 在终局：
dump_json(self.episode_transitions, path)
self.episode_transitions.clear()
```

### 9. 为什么 PPO 这里“看起来学得慢”
- 采样被人为延迟 → advantage 估计周期拉长。
- 每次 learn 只是 500 timesteps，小批次 + 高延迟 → 更新噪声大。
- 没有丰富对手分布（只有你一个人类）。

### 10. 如果不改会怎样
- 可以继续玩，但收益/时间比极低。
- 你可能花几小时，参数几乎没显著差异。

### 11. 推荐行动顺序（再次强调）
1. 立刻加轨迹记录（低风险，高价值）。
2. 写 imitation 训练脚本，产出一个“人类风格”初始策略。
3. 用该策略初始化 PPO，再做自对弈/混合真人互动微调。
4. 长期把所有对弈归档，后续探索离线强化学习。

（本节用于解释：为什么“真人对弈 != 当前实现的有效 RL 学习”，防止误解。）

